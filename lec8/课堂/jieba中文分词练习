import jieba
import jieba.analyse
f = open('aa.txt','r',encoding='utf-8')
text = f.read()
f.close()


----------


# 分词
seg_list = jieba.cut(text, cut_all=True)
print("Full Mode: " + "/ ".join(seg_list))  # 全模式

seg_list = jieba.cut(text, cut_all=False)
print("Default Mode: " + "/ ".join(seg_list))  # 精确模式

seg_list = jieba.cut_for_search(text)  # 搜索引擎模式
print(", ".join(seg_list))


----------


# 关键字提取
# 基于TF-IDF算法的关键词抽取
# sentence 为待提取的文本
# topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20
# withWeight 为是否一并返回关键词权重值，默认值为 False
# allowPOS 仅包括指定词性的词，默认值为空，即不筛选
keywords = jieba.analyse.extract_tags(sentence=text, topK=20, withWeight=True, allowPOS=('n','nr','ns'))
# 基于TextRank算法的关键词抽取
# keywords = jieba.analyse.textrank(text, topK=20, withWeight=True, allowPOS=('n','nr','ns'))
for item in keywords:
    print(item[0],item[1])


----------


# 词语标注
import jieba.posseg
# 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。
posseg = jieba.posseg.POSTokenizer(tokenizer=None)
words = posseg.cut(text)
for word, flag in words:
    print('%s %s' % (word, flag))